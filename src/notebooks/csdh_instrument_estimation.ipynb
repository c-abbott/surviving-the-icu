{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "institutional-separate",
   "metadata": {},
   "source": [
    "# The four steps of causal inference\n",
    "\n",
    "## I. Model a causal problem\n",
    "- Create a causal DAG for your system of interest.\n",
    "\n",
    "## II. Identify a target estimand under the model\n",
    "- Identify the causal estimand under the assumptions of the causal DAG.\n",
    "\n",
    "## III. Estimate the causal effect based on the identified estimand\n",
    "- Estimate the estimand using any kind of Stats/ML model e.g. linear regression, random forest etc.\n",
    "\n",
    "## IV. Refute the obtain estimate\n",
    "- Peform refutations on the estimate to test its robustness \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "given-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy as ps\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "import statsmodels\n",
    "\n",
    "import pygraphviz\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "import econml\n",
    "\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from econml.deepiv import DeepIVEstimator\n",
    "\n",
    "\n",
    "from econml.inference import BootstrapInference\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Avoid printing dataconversion warnings from sklearn\n",
    "# Config dict to set the logging level\n",
    "import logging.config\n",
    "DEFAULT_LOGGING = {\n",
    "    'version': 1,\n",
    "    'disable_existing_loggers': False,\n",
    "    'loggers': {\n",
    "        '': {\n",
    "            'level': 'WARN',\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "logging.config.dictConfig(DEFAULT_LOGGING)\n",
    "from IPython.display import Image, display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distributed-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = Path().resolve()\n",
    "PARENT_DIR = os.path.dirname(cwd)\n",
    "SCRIPT_DIR = os.path.join(PARENT_DIR, 'helpers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "younger-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(SCRIPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "marine-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dowhy_helpers as dwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "professional-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O Stuff\n",
    "DATA_FILENAME = \"csdh_clean.csv\"\n",
    "DATA_FILEPATH = \"/Users/callum/Uni/GitHubRepos/surviving-the-icu/datasets/drain_data/\" + DATA_FILENAME\n",
    "csdh = pd.read_csv(DATA_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30cba4",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Na√Øve Estimation (no causal inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0a7c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without adjusting for any confounding, the naive causal estimate is computed as -0.09356128931064231\n"
     ]
    }
   ],
   "source": [
    "naive_est = dwh.naive_estimate(df=csdh, treatment='drain', outcome='recurrence', treatment_type='int')\n",
    "print(f\"Without adjusting for any confounding, the naive causal estimate is computed as {naive_est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5439b",
   "metadata": {},
   "source": [
    "---\n",
    "## I. Model a causal problem\n",
    "\n",
    "### Note the `mp_model` is not included since this model contains to instruments \n",
    "* Create a causal model from the data and given graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf73950",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = CausalModel(data=csdh, \n",
    "                         treatment='drain', \n",
    "                         outcome='recurrence', \n",
    "                         graph='../causal_graphs/data_dag.dot'.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f085929",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data_model = CausalModel(data=csdh,\n",
    "                               treatment='drain', \n",
    "                               outcome='recurrence', \n",
    "                               graph='../causal_graphs/small_data_dag.dot'.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a31aa0",
   "metadata": {},
   "source": [
    "---\n",
    "## II. Identify a target estimand under the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a409543",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_estimand = data_model.identify_effect(proceed_when_unidentifiable=True)\n",
    "small_data_estimand = small_data_model.identify_effect(proceed_when_unidentifiable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-sight",
   "metadata": {},
   "source": [
    "---\n",
    "## III. Instrumental variable (IV; hospital) estimator\n",
    "\n",
    "### This estimate applies to the `data_model` and `small_data_model` only; `mp_model` contains no instruments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-antenna",
   "metadata": {},
   "source": [
    "The instrumental variables approach attempts to estimate the effect of $T$ on $Y$ with the help of a third variable  $Z$ that is correlated with $T$ but is uncorrelated with the error term for $Y$. In other words, the instrument $Z$ is only related with $Y$ through the directed path that goes through $T$. If these conditions are satisfied, the effect of $T$ on $Y$ can be estimated using the sample analog of:\n",
    "\n",
    "$$\\frac{Cov(Y_i, Z_i)}{Cov(T_i, Z_i)}$$\n",
    "\n",
    "The most common method for instrumental variables estimation is the two-stage least squares (2SLS). In this approach, the cause variable $T$ is first regressed on the instrument $Z$. Then, in the second stage, the outcome of interest $Y$ is regressed on the predicted value from the first-stage model. Intuitively, the effect of $T$ on $Y$ is estimated by using only the proportion of variation in $T$ due to variation in $Z$. See https://www.aeaweb.org/articles?id=10.1257/jep.15.4.69  for a detailed discussion of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "approved-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wald estimate method\n",
    "data_iv_est = data_model.estimate_effect(data_estimand,\n",
    "                                         control_value=0,\n",
    "                                         treatment_value=1,\n",
    "                                         method_name='iv.instrumental_variable',\n",
    "                                         method_params={'iv_instrument_name': 'hospital'},\n",
    "                                         test_significance=True, \n",
    "                                         confidence_intervals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a16eaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wald estimate method\n",
    "sdata_iv_est = small_data_model.estimate_effect(small_data_estimand,\n",
    "                                                control_value=0,\n",
    "                                                treatment_value=1,\n",
    "                                                method_name='iv.instrumental_variable',\n",
    "                                                method_params={'iv_instrument_name': 'hospital'},\n",
    "                                                test_significance=True, \n",
    "                                                confidence_intervals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fa06a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Causal Estimate ***\n",
      "\n",
      "## Identified estimand\n",
      "Estimand type: nonparametric-ate\n",
      "\n",
      "### Estimand : 1\n",
      "Estimand name: iv\n",
      "Estimand expression:\n",
      "Expectation(Derivative(recurrence, [hospital])*Derivative([drain], [hospital])\n",
      "**(-1))\n",
      "Estimand assumption 1, As-if-random: If U‚Üí‚Üírecurrence then ¬¨(U ‚Üí‚Üí{hospital})\n",
      "Estimand assumption 2, Exclusion: If we remove {hospital}‚Üí{drain}, then ¬¨({hospital}‚Üírecurrence)\n",
      "\n",
      "## Realized estimand\n",
      "Realized estimand: Wald Estimator\n",
      "Realized estimand type: nonparametric-ate\n",
      "Estimand expression:\n",
      "                                                                              \n",
      "Expectation(Derivative(recurrence, hospital))‚ãÖExpectation(Derivative(drain, ho\n",
      "\n",
      "        -1\n",
      "spital))  \n",
      "Estimand assumption 1, As-if-random: If U‚Üí‚Üírecurrence then ¬¨(U ‚Üí‚Üí{hospital})\n",
      "Estimand assumption 2, Exclusion: If we remove {hospital}‚Üí{drain}, then ¬¨({hospital}‚Üírecurrence)\n",
      "Estimand assumption 3, treatment_effect_homogeneity: Each unit's treatment ['drain'] is affected in the same way by common causes of ['drain'] and recurrence\n",
      "Estimand assumption 4, outcome_effect_homogeneity: Each unit's outcome recurrence is affected in the same way by common causes of ['drain'] and recurrence\n",
      "\n",
      "Target units: ate\n",
      "\n",
      "## Estimate\n",
      "Mean value: -0.28110039057421726\n",
      "p-value: 0.159\n",
      "95.0% confidence interval: (-0.8278650012508906, 0.19237918215613412)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_iv_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69167e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Causal Estimate ***\n",
      "\n",
      "## Identified estimand\n",
      "Estimand type: nonparametric-ate\n",
      "\n",
      "### Estimand : 1\n",
      "Estimand name: iv\n",
      "Estimand expression:\n",
      "Expectation(Derivative(recurrence, [hospital])*Derivative([drain], [hospital])\n",
      "**(-1))\n",
      "Estimand assumption 1, As-if-random: If U‚Üí‚Üírecurrence then ¬¨(U ‚Üí‚Üí{hospital})\n",
      "Estimand assumption 2, Exclusion: If we remove {hospital}‚Üí{drain}, then ¬¨({hospital}‚Üírecurrence)\n",
      "\n",
      "## Realized estimand\n",
      "Realized estimand: Wald Estimator\n",
      "Realized estimand type: nonparametric-ate\n",
      "Estimand expression:\n",
      "                                                                              \n",
      "Expectation(Derivative(recurrence, hospital))‚ãÖExpectation(Derivative(drain, ho\n",
      "\n",
      "        -1\n",
      "spital))  \n",
      "Estimand assumption 1, As-if-random: If U‚Üí‚Üírecurrence then ¬¨(U ‚Üí‚Üí{hospital})\n",
      "Estimand assumption 2, Exclusion: If we remove {hospital}‚Üí{drain}, then ¬¨({hospital}‚Üírecurrence)\n",
      "Estimand assumption 3, treatment_effect_homogeneity: Each unit's treatment ['drain'] is affected in the same way by common causes of ['drain'] and recurrence\n",
      "Estimand assumption 4, outcome_effect_homogeneity: Each unit's outcome recurrence is affected in the same way by common causes of ['drain'] and recurrence\n",
      "\n",
      "Target units: ate\n",
      "\n",
      "## Estimate\n",
      "Mean value: -0.28110039057421726\n",
      "p-value: 0.169\n",
      "95.0% confidence interval: (-0.7381280668315822, 0.2114614357605009)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sdata_iv_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b0ec3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>IV2SLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>recurrence</td>    <th>  R-squared:         </th> <td>  -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>IV2SLS</td>      <th>  Adj. R-squared:    </th> <td>  -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>Two Stage</td>    <th>  F-statistic:       </th> <td>   1.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                    <td>Least Squares</td>  <th>  Prob (F-statistic):</th>  <td> 0.302</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 05 Jul 2021</td> <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:22:50</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   745</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   743</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.3273</td> <td>    0.228</td> <td>    1.439</td> <td> 0.151</td> <td>   -0.119</td> <td>    0.774</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>drain</th>     <td>   -0.2811</td> <td>    0.272</td> <td>   -1.033</td> <td> 0.302</td> <td>   -0.815</td> <td>    0.253</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>352.300</td> <th>  Durbin-Watson:     </th> <td>   1.923</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1397.168</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.296</td>  <th>  Prob(JB):          </th> <td>4.06e-304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.891</td>  <th>  Cond. No.          </th> <td>    4.73</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          IV2SLS Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:             recurrence   R-squared:                      -0.043\n",
       "Model:                         IV2SLS   Adj. R-squared:                 -0.045\n",
       "Method:                     Two Stage   F-statistic:                     1.067\n",
       "                        Least Squares   Prob (F-statistic):              0.302\n",
       "Date:                Mon, 05 Jul 2021                                         \n",
       "Time:                        10:22:50                                         \n",
       "No. Observations:                 745                                         \n",
       "Df Residuals:                     743                                         \n",
       "Df Model:                           1                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.3273      0.228      1.439      0.151      -0.119       0.774\n",
       "drain         -0.2811      0.272     -1.033      0.302      -0.815       0.253\n",
       "==============================================================================\n",
       "Omnibus:                      352.300   Durbin-Watson:                   1.923\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1397.168\n",
       "Skew:                           2.296   Prob(JB):                    4.06e-304\n",
       "Kurtosis:                       7.891   Cond. No.                         4.73\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two-Stage Least-Squares (2SLS) Regression\n",
    "# We see congruence between this and the DoWhy method\n",
    "rec_vec, endog = ps.dmatrices(\"recurrence ~ drain\", data=csdh)\n",
    "exog = ps.dmatrix(\"hospital\", data=csdh)\n",
    "\n",
    "m = IV2SLS(rec_vec, endog, exog).fit()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a9e05",
   "metadata": {},
   "source": [
    "## For IV methods, only the treatment, outcome and instrument are important; the rest of the causal graph is ignored. This explains why firstly the instrumental variable method differs so greatly from other estimation techniques and why the `data_model` and `small_data_model` are perfectly congruent.\n",
    "\n",
    "### We carry on the rest of this notebook using the `small_data_model` since it is more lightweight and hence runs faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b876065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Causal Estimates -------------- \n",
      "Naive causal estimate is -0.09356128931064231\n",
      "Instrumental variable causal estimate is -0.28110039057421726\n",
      "Percentage change from naive_est: 200.445%\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# small data model\n",
    "dwh.print_estimate_comparison(naive_est, sdata_iv_est, 'Instrumental variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "434c0366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Causal Estimates -------------- \n",
      "Naive causal estimate is -0.09356128931064231\n",
      "Instrumental variable causal estimate is -0.28110039057421726\n",
      "Percentage change from naive_est: 200.445%\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# data model\n",
    "dwh.print_estimate_comparison(naive_est, data_iv_est, 'Instrumental variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e9505",
   "metadata": {},
   "source": [
    "---\n",
    "## IV. Refute the obtained estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45cd0d2",
   "metadata": {},
   "source": [
    "1. **Add Random Common Cause:** Does the estimation method change its estimate after we add an independent random variable as a common cause to the dataset? (Hint: It should not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9943a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust if: estimate stays the same\n",
    "iv_ran_refuter = small_data_model.refute_estimate(small_data_estimand,\n",
    "                                                  sdata_iv_est,\n",
    "                                                  num_simulations=100,\n",
    "                                                  method_name=\"random_common_cause\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f972c",
   "metadata": {},
   "source": [
    "2. **Placebo Treatment:** What happens to the estimated causal effect when we replace the true treatment variable with an independent random variable? (Hint: the effect should go to zero)\n",
    "- Note that the placebo type is 'permute' meaning the rows of the treatment variable have been randomly permuted giving the effect of a placebo treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f10cbeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust if: estimate goes to 0\n",
    "iv_placebo_refuter = small_data_model.refute_estimate(small_data_estimand, \n",
    "                                                      sdata_iv_est,\n",
    "                                                      method_name=\"placebo_treatment_refuter\",\n",
    "                                                      num_simulations=100,\n",
    "                                                      placebo_type='permute')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f256132",
   "metadata": {},
   "source": [
    "3. **Dummy Outcome:** What happens to the estimated causal effect when we replace the true outcome variable with an independent random variable? (Hint: The effect should go to zero)\n",
    "\n",
    "- The result shows that when using a dummy outcome, the **treatment does not lead to the outcome**. The estimated effect is hence a value that tends to zero, which matches our expectation. This shows that if we replace the outcome by randomly generated data, the **estimator correctly predicts that the influence if treatment is zero**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d45b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOT APPLICABLE TO IV ESTIMATION METHODS ########\n",
    "# Robust if: estimate goes to 0\n",
    "# iv_dummy_refuter = data_model.refute_estimate(data_estimand,\n",
    "#                                               data_iv_est,\n",
    "#                                               method_name=\"dummy_outcome_refuter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ebe829",
   "metadata": {},
   "source": [
    "4. **Data Subsets Validation:** Does the estimated effect change significantly when we replace the given dataset with a randomly selected subset? (Hint: It should not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67e813ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust if: estimate stays the same\n",
    "iv_subset_refuter = small_data_model.refute_estimate(small_data_estimand, \n",
    "                                                     sdata_iv_est,\n",
    "                                                     method_name=\"data_subset_refuter\",\n",
    "                                                     num_simulations=100,\n",
    "                                                     subset_fraction=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4620bb5c",
   "metadata": {},
   "source": [
    "5. **Bootstrap Validation:** Does the estimated effect change significantly when we replace the given dataset with bootstrapped samples from the same dataset? (Hint: It should not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "106fb977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust if: estimate stays the same\n",
    "iv_bootstrap_refuter = small_data_model.refute_estimate(small_data_estimand, \n",
    "                                                        sdata_iv_est,\n",
    "                                                        method_name=\"bootstrap_refuter\", \n",
    "                                                        num_simulations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa9d22af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refute: Add a Random Common Cause\n",
      "Estimated effect:-0.28110039057421726\n",
      "New effect:-0.28110039057421726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iv_ran_refuter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4d4fc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refute: Use a Placebo Treatment\n",
      "Estimated effect:-0.28110039057421726\n",
      "New effect:0.003687280619944968\n",
      "p value:0.49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iv_placebo_refuter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d502903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refute: Use a subset of data\n",
      "Estimated effect:-0.28110039057421726\n",
      "New effect:-0.297031230083872\n",
      "p value:0.48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iv_subset_refuter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5ebedce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refute: Bootstrap Sample Dataset\n",
      "Estimated effect:-0.28110039057421726\n",
      "New effect:-0.25066498481613725\n",
      "p value:0.41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iv_bootstrap_refuter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ae081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from econml.deepiv import DeepIVEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/csdh/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/csdh/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 633 samples, validate on 112 samples\n",
      "Epoch 1/500\n",
      "633/633 [==============================] - 1s 2ms/step - loss: 1.0601 - val_loss: 0.3046\n",
      "Epoch 2/500\n",
      "633/633 [==============================] - 0s 134us/step - loss: 0.5468 - val_loss: 0.1953\n",
      "Epoch 3/500\n",
      "633/633 [==============================] - 0s 129us/step - loss: 0.4040 - val_loss: 0.1711\n",
      "Epoch 4/500\n",
      "633/633 [==============================] - 0s 128us/step - loss: 0.3111 - val_loss: 0.1218\n",
      "Epoch 5/500\n",
      "633/633 [==============================] - 0s 129us/step - loss: 0.2368 - val_loss: 0.0102\n",
      "Epoch 6/500\n",
      "633/633 [==============================] - 0s 124us/step - loss: 0.2272 - val_loss: -0.0178\n",
      "Epoch 7/500\n",
      "633/633 [==============================] - 0s 122us/step - loss: 0.1373 - val_loss: -0.0545\n",
      "Epoch 8/500\n",
      "633/633 [==============================] - 0s 123us/step - loss: 0.0445 - val_loss: -0.1733\n",
      "Epoch 9/500\n",
      "633/633 [==============================] - 0s 121us/step - loss: 0.0052 - val_loss: -0.1884\n",
      "Epoch 10/500\n",
      "633/633 [==============================] - 0s 122us/step - loss: -0.0793 - val_loss: -0.3282\n",
      "Epoch 11/500\n",
      "633/633 [==============================] - 0s 122us/step - loss: -0.1777 - val_loss: -0.3842\n",
      "Epoch 12/500\n",
      "633/633 [==============================] - 0s 123us/step - loss: -0.2286 - val_loss: -0.4917\n",
      "Epoch 13/500\n",
      "633/633 [==============================] - 0s 124us/step - loss: -0.2721 - val_loss: -0.4744\n",
      "Epoch 14/500\n",
      "633/633 [==============================] - 0s 125us/step - loss: -0.3324 - val_loss: -0.4981\n",
      "Epoch 15/500\n",
      "633/633 [==============================] - 0s 121us/step - loss: -0.3804 - val_loss: -0.7644\n",
      "Epoch 16/500\n",
      "633/633 [==============================] - 0s 120us/step - loss: -0.3653 - val_loss: -0.5590\n",
      "Epoch 17/500\n",
      "633/633 [==============================] - 0s 124us/step - loss: -0.4220 - val_loss: -0.5508\n"
     ]
    }
   ],
   "source": [
    "dims_zx = len(small_data_model._instruments)+len(small_data_model._effect_modifiers)\n",
    "dims_tx = len(small_data_model._treatment)+len(small_data_model._effect_modifiers)\n",
    "\n",
    "treatment_model = keras.Sequential([keras.layers.Dense(128, activation='relu', input_shape=(dims_zx,)), # sum of dims of Z and X\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(64, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(26, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17)])\n",
    "\n",
    "response_model = keras.Sequential([keras.layers.Dense(128, activation='relu', input_shape=(dims_tx,)), # sum of dims of T and X\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(64, activation='relu'),\n",
    "                                   keras.layers.Dense(32, activation='relu'),\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "small_deepiv_estimate = small_data_model.estimate_effect(small_data_estimand,\n",
    "                                                   method_name=\"iv.econml.iv.nnet.DeepIV\",\n",
    "                                                   control_value=0,\n",
    "                                                   treatment_value=1,\n",
    "                                                   confidence_intervals=True,\n",
    "                                                   target_units='ate',\n",
    "                                             method_params={\"init_params\":{'n_components': 10, # Number of gaussians in the mixture density networks\n",
    "                                                              'm': lambda z, x: treatment_model(keras.layers.concatenate([z, x])), # Treatment model,\n",
    "                                                              \"h\": lambda t, x: response_model(keras.layers.concatenate([t, x])), # Response model\n",
    "                                                              'n_samples': 100, # Number of samples used to estimate the response\n",
    "                                                              'first_stage_options': {'epochs':500,\n",
    "                                                                                      \"callbacks\": [keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "                                                                                      'validation_split':0.15\n",
    "                                                                                     },\n",
    "                                                              'second_stage_options': {'epochs':100,\n",
    "                                                                                       \"callbacks\": [keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)],\n",
    "                                                                                       'validation_split':0.15\n",
    "                                                                                      }},\n",
    "                                                            \"fit_params\":{\n",
    "                                                                'inference': BootstrapInference(n_bootstrap_samples=100, n_jobs=-1),\n",
    "                                                            }\n",
    "                                                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_deepiv_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade04c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
